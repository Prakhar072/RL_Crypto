{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6a2b868-b3fd-484a-a1ea-247d3087c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a31dd9-6828-40d8-9657-0ba6526614b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://data.binance.vision/data/spot/daily/klines/ETHUSDT/1s/\"\n",
    "\n",
    "start_date = datetime(2017, 8, 17) \n",
    "end_date = datetime(2024, 8, 28)\n",
    "\n",
    "file_format = \"zip\"\n",
    "\n",
    "DOWNLOAD_DIR=\"binance_data\"\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "EXTRACT_DIR = os.path.join(DOWNLOAD_DIR, \"extracted_csv\")\n",
    "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
    "MERGED_DIR = os.path.join(DOWNLOAD_DIR, \"merged_data\")\n",
    "os.makedirs(MERGED_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03cefbda-3b27-408b-aa2c-6cb4739989d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████████████████████████████████████████████████████████| 2569/2569 [00:00<00:00, 10681.65file/s]\n"
     ]
    }
   ],
   "source": [
    "dates = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n",
    "file_list = [f\"ETHUSDT-1s-{date.strftime('%Y-%m-%d')}.{file_format}\" for date in dates]\n",
    "\n",
    "def download_file(file_name):\n",
    "    file_url = BASE_URL + file_name\n",
    "    save_path = os.path.join(DOWNLOAD_DIR, file_name)\n",
    "    \n",
    "    if os.path.exists(save_path):\n",
    "        return file_name  # Skip if already downloaded\n",
    "    \n",
    "    for attempt in range(3):  # Retry up to 3 times\n",
    "        try:\n",
    "            response = requests.get(file_url, stream=True, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                with open(save_path, \"wb\") as f:\n",
    "                    for chunk in response.iter_content(chunk_size=1024):\n",
    "                        f.write(chunk)\n",
    "                return file_name  # Successfully downloaded\n",
    "            else:\n",
    "                print(f\"Failed: {file_name}, Status Code: {response.status_code}\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error downloading {file_name}: {e}\")\n",
    "    return None  # Failed after retries\n",
    "\n",
    "# Download with threading\n",
    "with tqdm(total=len(file_list), desc=\"Downloading\", unit=\"file\") as pbar:\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(download_file, file): file for file in file_list}\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            if future.result():\n",
    "                pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3bb6a6b-095b-43eb-8216-b89c4f47816f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files: 100%|██████████████████████████████████████████████████████████| 2569/2569 [09:43<00:00,  4.41file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files extracted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "zip_files = glob.glob(os.path.join(DOWNLOAD_DIR, \"*.zip\"))\n",
    "\n",
    "# Single progress bar for total extractions\n",
    "with tqdm(total=len(zip_files), desc=\"Extracting Files\", unit=\"file\") as pbar:\n",
    "    for zip_file in zip_files:\n",
    "        with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(EXTRACT_DIR)\n",
    "        pbar.update(1)  # Update progress bar after each extraction\n",
    "\n",
    "print(\"All files extracted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e2f779-fa70-446c-b27a-7f8ed94b4689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging CSV Files: 100%|█████████████████████████████████████████████████████████| 1233/1233 [03:12<00:00,  6.40file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged file: binance_data\\merged_data\\ETHUSDT_1s_2017_2020.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging CSV Files: 100%|█████████████████████████████████████████████████████████| 1336/1336 [03:54<00:00,  5.71file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged file: binance_data\\merged_data\\ETHUSDT_1s_2021_2024.csv\n"
     ]
    }
   ],
   "source": [
    "def extract_date(filename):\n",
    "    match = re.search(r\"(\\d{4}-\\d{2}-\\d{2})\", filename)  # Extracts YYYY-MM-DD\n",
    "    return match.group(1) if match else \"\"\n",
    "\n",
    "def merge_csv(file_list, output_file):\n",
    "    df_list = []\n",
    "    \n",
    "    # Progress bar setup\n",
    "    with tqdm(total=len(file_list), desc=\"Merging CSV Files\", unit=\"file\") as pbar:\n",
    "        for file in file_list:\n",
    "            df = pd.read_csv(file, compression=None, header=None)  # No headers\n",
    "            df_list.append(df)\n",
    "            pbar.update(1)  # Update progress bar\n",
    "\n",
    "    if not df_list:\n",
    "        print(f\"No valid data for {output_file}. Skipping...\")\n",
    "        return\n",
    "    \n",
    "    merged_df = pd.concat(df_list, ignore_index=True)\n",
    "    merged_df = merged_df.sort_values(by=0)  # Sort by timestamp (assuming first column is timestamp)\n",
    "    merged_df.to_csv(output_file, index=False, header=False)\n",
    "    \n",
    "    print(f\"Saved merged file: {output_file}\")\n",
    "\n",
    "# Get list of extracted CSV files\n",
    "csv_files = glob.glob(os.path.join(EXTRACT_DIR, \"*.csv\"))\n",
    "\n",
    "# Sort CSV files by date in filename\n",
    "csv_files_sorted = sorted(csv_files, key=extract_date)\n",
    "\n",
    "# Filter based on years\n",
    "csv_2017_2020 = [f for f in csv_files_sorted if \"2017\" in f or \"2018\" in f or \"2019\" in f or \"2020\" in f]\n",
    "csv_2021_2024 = [f for f in csv_files_sorted if \"2021\" in f or \"2022\" in f or \"2023\" in f or \"2024\" in f]\n",
    "\n",
    "# Define output filenames\n",
    "output_file_1 = os.path.join(MERGED_DIR, \"ETHUSDT_1s_2017_2020.csv\")\n",
    "output_file_2 = os.path.join(MERGED_DIR, \"ETHUSDT_1s_2021_2024.csv\")\n",
    "\n",
    "# Merge CSV files with progress bar\n",
    "merge_csv(csv_2017_2020, output_file_1)\n",
    "merge_csv(csv_2021_2024, output_file_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fb59b79-694f-4c05-914b-c919f0041cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed and saved: binance_data\\merged_data\\ETHUSDT_1s_2017_2020.csv.gz\n",
      "Compressed and saved: binance_data\\merged_data\\ETHUSDT_1s_2021_2024.csv.gz\n",
      "Deleted all intermediate files.\n"
     ]
    }
   ],
   "source": [
    "def compress_csv(file_path):\n",
    "    compressed_path = file_path + \".gz\"\n",
    "    with open(file_path, 'rb') as f_in, open(compressed_path, 'wb') as f_out:\n",
    "        f_out.write(f_in.read())\n",
    "    os.remove(file_path)  # Delete original CSV\n",
    "    print(f\"Compressed and saved: {compressed_path}\")\n",
    "\n",
    "compress_csv(output_file_1)\n",
    "compress_csv(output_file_2)\n",
    "\n",
    "# Delete all intermediate files\n",
    "for file in zip_files + csv_files_sorted:\n",
    "    os.remove(file)\n",
    "print(\"Deleted all intermediate files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9346f150-3024-49f0-b60c-48679f5360da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed: binance_data\\merged_data\\ETHUSDT_1s_2017_2020.csv.gz and removed binance_data\\merged_data\\ETHUSDT_1s_2017_2020.csv\n",
      "Compressed: binance_data\\merged_data\\ETHUSDT_1s_2021_2024.csv.gz and removed binance_data\\merged_data\\ETHUSDT_1s_2021_2024.csv\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "def compress_and_cleanup(csv_file, directory, buffer_size=1024*1024):\n",
    "    \"\"\"Compress a CSV file in the given directory to .csv.gz using buffering and delete the original.\"\"\"\n",
    "    csv_path = os.path.join(directory, csv_file)\n",
    "    gz_path = csv_path + \".gz\"\n",
    "\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"File not found: {csv_path}\")\n",
    "        return\n",
    "\n",
    "    with open(csv_path, 'rb') as f_in, gzip.open(gz_path, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out, length=buffer_size)\n",
    "\n",
    "    # os.remove(csv_path)  # Delete original CSV file\n",
    "    print(f\"Compressed: {gz_path} and removed {csv_path}\")\n",
    "\n",
    "compress_and_cleanup(\"ETHUSDT_1s_2017_2020.csv\", MERGED_DIR)\n",
    "compress_and_cleanup(\"ETHUSDT_1s_2021_2024.csv\", MERGED_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382bd461-5e1a-4dc2-9982-f8b84c32bd25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
